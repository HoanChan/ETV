
# So sÃ¡nh DecoderLayer vÃ  TMDLayer

ThÆ° má»¥c nÃ y chá»©a so sÃ¡nh toÃ n diá»‡n giá»¯a hai lá»›p decoder:
- `DecoderLayer`: CÃ i Ä‘áº·t tÃ¹y chá»‰nh báº±ng PyTorch thuáº§n
- `TMDLayer`: CÃ i Ä‘áº·t dá»±a trÃªn MMCV sá»­ dá»¥ng `BaseTransformerLayer`

## Tá»•ng quan cÃ¡c tá»‡p

- `decoder_layer.py`: Lá»›p decoder tÃ¹y chá»‰nh vá»›i attention Ä‘a Ä‘áº§u
- `tmd_layer.py`: Lá»›p decoder transformer dá»±a trÃªn MMCV
- `test_decoder_vs_tmd.py`: Bá»™ kiá»ƒm thá»­ so sÃ¡nh hai cÃ i Ä‘áº·t
- `usage_examples.py`: VÃ­ dá»¥ sá»­ dá»¥ng thá»±c táº¿ vÃ  hÆ°á»›ng dáº«n chuyá»ƒn Ä‘á»•i
- `test_requirements.txt`: CÃ¡c phá»¥ thuá»™c bá»• sung cho kiá»ƒm thá»­

## Káº¿t quáº£ chÃ­nh

### âœ… TÆ°Æ¡ng thÃ­ch
- **KÃ­ch thÆ°á»›c Ä‘áº§u ra**: Cáº£ hai lá»›p Ä‘á»u cho Ä‘áº§u ra cÃ¹ng kÃ­ch thÆ°á»›c
- **Sá»‘ lÆ°á»£ng tham sá»‘**: Giá»‘ng nhau (4.204.032)
- **á»”n Ä‘á»‹nh sá»‘ há»c**: á»”n Ä‘á»‹nh trÃªn nhiá»u dáº£i giÃ¡ trá»‹ Ä‘áº§u vÃ o
- **Lan truyá»n gradient**: Gradient há»£p lá»‡, Ä‘á»™ lá»›n tÆ°Æ¡ng Ä‘Æ°Æ¡ng

### âš¡ Hiá»‡u nÄƒng
- **DecoderLayer**: Nhanh hÆ¡n ~1.2 láº§n trung bÃ¬nh
- **TMDLayer**: Cháº­m hÆ¡n má»™t chÃºt nhÆ°ng chuáº©n hÃ³a hÆ¡n
- **Bá»™ nhá»›**: Gáº§n nhÆ° giá»‘ng nhau (~16MB, chÃªnh lá»‡ch < 0.1MB)

### ğŸ”§ KhÃ¡c biá»‡t ká»¹ thuáº­t

| KhÃ­a cáº¡nh | DecoderLayer | TMDLayer |
|----------|--------------|----------|
| **Phá»¥ thuá»™c** | PyTorch thuáº§n | Cáº§n MMCV |
| **Äá»‹nh dáº¡ng mask** | Tensor 4D | Tensor 2D |
| **Cáº¥u hÃ¬nh** | Dáº¡ng dict | Tham sá»‘ trá»±c tiáº¿p |
| **Linh hoáº¡t** | Attention tÃ¹y chá»‰nh | Chuáº©n hÃ³a |
| **TÃ­ch há»£p** | Äá»™c láº­p | Há»‡ sinh thÃ¡i MMCV |

## VÃ­ dá»¥ sá»­ dá»¥ng

### DecoderLayer
```python
layer = DecoderLayer(
    size=512,
    self_attn={'headers': 8, 'd_model': 512, 'dropout': 0.1},
    src_attn={'headers': 8, 'd_model': 512, 'dropout': 0.1},
    feed_forward={'d_model': 512, 'd_ff': 2048, 'dropout': 0.1},
    dropout=0.1
)

output = layer(target, source, src_mask, tgt_mask)
```

### TMDLayer
```python
layer = TMDLayer(
    d_model=512,
    n_head=8,
    d_inner=2048,
    attn_drop=0.1,
    ffn_drop=0.1
)

output = layer(
    query=target,
    key=source,
    value=source,
    query_pos=None,
    key_pos=None,
    attn_masks=[self_attn_mask, cross_attn_mask],
    query_key_padding_mask=None,
    key_padding_mask=None
)
```

## Cháº¡y kiá»ƒm thá»­

### YÃªu cáº§u
```bash
pip install torch mmcv-full psutil
```

### Cháº¡y toÃ n bá»™ kiá»ƒm thá»­
```bash
python test_decoder_vs_tmd.py
```

### Cháº¡y vÃ­ dá»¥ sá»­ dá»¥ng
```bash
python usage_examples.py
```

## TÃ³m táº¯t káº¿t quáº£ kiá»ƒm thá»­

```
============================================================
SO SÃNH DecoderLayer vs TMDLayer
============================================================
Kiá»ƒm tra kÃ­ch thÆ°á»›c Ä‘áº§u ra...
âœ“ DecoderLayer output shape: torch.Size([2, 10, 512])
âœ“ TMDLayer output shape: torch.Size([2, 10, 512])
âœ“ KÃ­ch thÆ°á»›c khá»›p: True

Kiá»ƒm tra sá»‘ lÆ°á»£ng tham sá»‘...
âœ“ DecoderLayer parameters: 4,204,032
âœ“ TMDLayer parameters: 4,204,032
âœ“ ChÃªnh lá»‡ch tham sá»‘: 0

Kiá»ƒm tra tÆ°Æ¡ng thÃ­ch forward...
âœ“ Táº¥t cáº£ kiá»ƒm thá»­ forward Ä‘á»u vÆ°á»£t qua!

Kiá»ƒm tra gradient...
âœ“ Cáº£ hai Ä‘á»u cÃ³ gradient khÃ¡c 0: True

Kiá»ƒm tra bá»™ nhá»›...
âœ“ ChÃªnh lá»‡ch bá»™ nhá»›: 0.04 MB

Kiá»ƒm tra hiá»‡u nÄƒng...
âœ“ DecoderLayer nhanh hÆ¡n 1.23x

Kiá»ƒm tra á»•n Ä‘á»‹nh sá»‘ há»c...
âœ“ Cáº£ hai Ä‘á»u á»•n Ä‘á»‹nh sá»‘ há»c
```

## HÆ°á»›ng dáº«n chuyá»ƒn Ä‘á»•i

### Tá»« DecoderLayer sang TMDLayer

1. **Cáº­p nháº­t cáº¥u hÃ¬nh**:
   - Äá»•i tá»« dict sang tham sá»‘ trá»±c tiáº¿p
   - Äá»•i tÃªn `headers` thÃ nh `n_head`
   - Äá»•i tÃªn `d_ff` thÃ nh `d_inner`

2. **Cáº­p nháº­t forward**:
   - DÃ¹ng tham sá»‘ tÃªn thay vÃ¬ vá»‹ trÃ­
   - Äá»•i mask tá»« 4D sang 2D
   - Äáº£o ngÆ°á»£c logic mask self-attention

3. **Cáº­p nháº­t phá»¥ thuá»™c**:
   - ThÃªm MMCV vÃ o requirements
   - Cáº­p nháº­t import

### Tá»« TMDLayer sang DecoderLayer

1. **Bá» phá»¥ thuá»™c MMCV**:
   - CÃ i Ä‘áº·t PyTorch thuáº§n
   - KhÃ´ng phá»¥ thuá»™c ngoÃ i

2. **Cáº­p nháº­t mask**:
   - Chuyá»ƒn mask 2D sang 4D
   - Äiá»u chá»‰nh logic mask

3. **Cáº­p nháº­t cáº¥u hÃ¬nh**:
   - DÃ¹ng dict cho cáº¥u hÃ¬nh
   - Äiá»u chá»‰nh tÃªn tham sá»‘

## Khuyáº¿n nghá»‹

### DÃ¹ng DecoderLayer khi:
- Muá»‘n giáº£m phá»¥ thuá»™c
- Cáº§n attention tÃ¹y chá»‰nh
- YÃªu cáº§u hiá»‡u nÄƒng cao
- LÃ m viá»‡c vá»›i PyTorch thuáº§n

### DÃ¹ng TMDLayer khi:
- ÄÃ£ dÃ¹ng há»‡ sinh thÃ¡i MMCV
- Muá»‘n chuáº©n hÃ³a thao tÃ¡c
- Cáº§n tÃ­ch há»£p tá»‘t vá»›i MMCV
- Æ¯u tiÃªn cÃ i Ä‘áº·t Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm thá»­ ká»¹

## Chi tiáº¿t kiáº¿n trÃºc

### Kiáº¿n trÃºc DecoderLayer
```
Input â†’ LayerNorm â†’ Self-Attention â†’ Residual â†’
        LayerNorm â†’ Cross-Attention â†’ Residual â†’
        LayerNorm â†’ FeedForward â†’ Residual â†’ Output
```

### Kiáº¿n trÃºc TMDLayer
```
Input â†’ Self-Attention â†’ LayerNorm â†’ Residual â†’
        Cross-Attention â†’ LayerNorm â†’ Residual â†’
        FeedForward â†’ LayerNorm â†’ Residual â†’ Output
```

Hai kiáº¿n trÃºc nÃ y vá» chá»©c nÄƒng lÃ  tÆ°Æ¡ng Ä‘Æ°Æ¡ng, chá»‰ khÃ¡c thá»© tá»± chuáº©n hÃ³a.

## ÄÃ¡nh giÃ¡ hiá»‡u nÄƒng

### Hiá»‡u nÄƒng lá»›p Ä‘Æ¡n
- **DecoderLayer**: ~4.1ms má»—i láº§n forward
- **TMDLayer**: ~5.1ms má»—i láº§n forward
- **TÄƒng tá»‘c**: DecoderLayer nhanh hÆ¡n 1.23x

### Hiá»‡u nÄƒng nhiá»u lá»›p (6 lá»›p)
- **DecoderLayer**: ~63.7ms má»—i láº§n forward
- **TMDLayer**: ~72.8ms má»—i láº§n forward
- **TÄƒng tá»‘c**: DecoderLayer nhanh hÆ¡n 1.14x

## Káº¿t luáº­n

Cáº£ hai cÃ i Ä‘áº·t Ä‘á»u ráº¥t tÆ°Æ¡ng thÃ­ch vÃ  cho káº¿t quáº£ giá»‘ng nhau. Viá»‡c lá»±a chá»n phá»¥ thuá»™c vÃ o nhu cáº§u cá»¥ thá»ƒ:

- **DecoderLayer**: PhÃ¹ há»£p á»©ng dá»¥ng cáº§n hiá»‡u nÄƒng, Ã­t phá»¥ thuá»™c
- **TMDLayer**: PhÃ¹ há»£p tÃ­ch há»£p há»‡ sinh thÃ¡i MMCV, thao tÃ¡c chuáº©n hÃ³a

Bá»™ kiá»ƒm thá»­ Ä‘áº£m báº£o cáº£ hai cÃ i Ä‘áº·t luÃ´n tÆ°Æ¡ng thÃ­ch vÃ  cÃ³ thá»ƒ thay tháº¿ cho nhau trong háº§u háº¿t trÆ°á»ng há»£p.
